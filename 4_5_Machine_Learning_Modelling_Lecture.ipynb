{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4.5_Machine_Learning_Modelling_Lecture.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gtoubian/cce/blob/main/4_5_Machine_Learning_Modelling_Lecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHKwBTvS_YEp"
      },
      "source": [
        "#4.4 Modelling\n",
        "\n",
        "In today's lecture, we will be going over an example of using a Machine Learning Model to run a predictive analysis. We will be working with the linnerud dataset from sklearn and we are going to create a model that will try to predict someone's waist size from information on the number of chinups, Situps and Jumping Jacks  they are able to perform in a fixed time period."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7C4CRWcNN71"
      },
      "source": [
        "from sklearn.datasets import load_linnerud\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjnzAreaNTGe",
        "outputId": "59fe1f58-9c26-4e7c-f5a6-6c1aa98414e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "data = load_linnerud()\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Chins</th>\n",
              "      <th>Situps</th>\n",
              "      <th>Jumps</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.0</td>\n",
              "      <td>162.0</td>\n",
              "      <td>60.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>60.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>12.0</td>\n",
              "      <td>101.0</td>\n",
              "      <td>101.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>12.0</td>\n",
              "      <td>105.0</td>\n",
              "      <td>37.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>13.0</td>\n",
              "      <td>155.0</td>\n",
              "      <td>58.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Chins  Situps  Jumps\n",
              "0    5.0   162.0   60.0\n",
              "1    2.0   110.0   60.0\n",
              "2   12.0   101.0  101.0\n",
              "3   12.0   105.0   37.0\n",
              "4   13.0   155.0   58.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqCJUpBufb3Q"
      },
      "source": [
        "When creating a model, we want to split our data into Training and Test Sets. The training set is essentially the set of data we use to calculate our model parameters using Ordinary Least Squares (OLS) and our test set is the set of data we use to see how well our model holds up given new data that it hasn't seen before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yydZVFtNuwL",
        "outputId": "17c21d99-a080-447b-e06b-c9c016070548",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y = data.target\n",
        "print(y)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[191.  36.  50.]\n",
            " [189.  37.  52.]\n",
            " [193.  38.  58.]\n",
            " [162.  35.  62.]\n",
            " [189.  35.  46.]\n",
            " [182.  36.  56.]\n",
            " [211.  38.  56.]\n",
            " [167.  34.  60.]\n",
            " [176.  31.  74.]\n",
            " [154.  33.  56.]\n",
            " [169.  34.  50.]\n",
            " [166.  33.  52.]\n",
            " [154.  34.  64.]\n",
            " [247.  46.  50.]\n",
            " [193.  36.  46.]\n",
            " [202.  37.  62.]\n",
            " [176.  37.  54.]\n",
            " [157.  32.  52.]\n",
            " [156.  33.  54.]\n",
            " [138.  33.  68.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KooJKyT2U-Xb",
        "outputId": "bdae221a-77a3-4e7a-d24e-c91016ba958c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y = data.target\n",
        "y = y[:, 1]\n",
        "print(y)\n",
        "x = df.to_numpy()\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[36. 37. 38. 35. 35. 36. 38. 34. 31. 33. 34. 33. 34. 46. 36. 37. 37. 32.\n",
            " 33. 33.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SU97pFwY_SS-"
      },
      "source": [
        "# y = a0 + a1*x1 + a2*x2 + a3*x3\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=4)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32DrUbjlUbU5",
        "outputId": "57813b20-6fbb-49a1-cfcc-747480d4fa34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = LinearRegression().fit(x_train, y_train)\n",
        "print(model.intercept_)\n",
        "print(model.coef_)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "38.935656643997255\n",
            "[-0.14675609 -0.02775666  0.02099938]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L32Uky0Oh9pW"
      },
      "source": [
        "The Scoring Function gives us an $R^2$ value for our model with the given parameters that it was trained with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHi76jHAPQgU",
        "outputId": "42b38840-fb3c-4ec0-c73f-8ea78082d140",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# R2 - 1 - SSres/SStot\n",
        "print(model.score(x_train, y_train))\n",
        "print(model.score(x_test, y_test))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6383894255040209\n",
            "0.3712452375606098\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szLvhP5HPChv",
        "outputId": "18b68db9-7ae0-407d-a71b-bde2643bb28d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        }
      },
      "source": [
        "import statsmodels.api as sm\n",
        "est = sm.OLS(y, x).fit()\n",
        "est.summary()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<table class=\"simpletable\">\n",
              "<caption>OLS Regression Results</caption>\n",
              "<tr>\n",
              "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared (uncentered):</th>      <td>   0.805</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared (uncentered):</th> <td>   0.771</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>          <td>   23.45</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Date:</th>             <td>Mon, 22 Mar 2021</td> <th>  Prob (F-statistic):</th>          <td>2.83e-06</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Time:</th>                 <td>14:36:21</td>     <th>  Log-Likelihood:    </th>          <td> -83.423</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>No. Observations:</th>      <td>    20</td>      <th>  AIC:               </th>          <td>   172.8</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Df Residuals:</th>          <td>    17</td>      <th>  BIC:               </th>          <td>   175.8</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>              <td> </td>   \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>              <td> </td>   \n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "   <td></td>     <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>x1</th> <td>    0.2759</td> <td>    1.024</td> <td>    0.269</td> <td> 0.791</td> <td>   -1.885</td> <td>    2.436</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>x2</th> <td>    0.1919</td> <td>    0.084</td> <td>    2.278</td> <td> 0.036</td> <td>    0.014</td> <td>    0.370</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>x3</th> <td>   -0.0154</td> <td>    0.102</td> <td>   -0.151</td> <td> 0.882</td> <td>   -0.231</td> <td>    0.200</td>\n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "  <th>Omnibus:</th>       <td> 0.745</td> <th>  Durbin-Watson:     </th> <td>   1.584</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Prob(Omnibus):</th> <td> 0.689</td> <th>  Jarque-Bera (JB):  </th> <td>   0.683</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Skew:</th>          <td> 0.110</td> <th>  Prob(JB):          </th> <td>   0.711</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Kurtosis:</th>      <td> 2.122</td> <th>  Cond. No.          </th> <td>    47.8</td>\n",
              "</tr>\n",
              "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
            ],
            "text/plain": [
              "<class 'statsmodels.iolib.summary.Summary'>\n",
              "\"\"\"\n",
              "                                 OLS Regression Results                                \n",
              "=======================================================================================\n",
              "Dep. Variable:                      y   R-squared (uncentered):                   0.805\n",
              "Model:                            OLS   Adj. R-squared (uncentered):              0.771\n",
              "Method:                 Least Squares   F-statistic:                              23.45\n",
              "Date:                Mon, 22 Mar 2021   Prob (F-statistic):                    2.83e-06\n",
              "Time:                        14:36:21   Log-Likelihood:                         -83.423\n",
              "No. Observations:                  20   AIC:                                      172.8\n",
              "Df Residuals:                      17   BIC:                                      175.8\n",
              "Df Model:                           3                                                  \n",
              "Covariance Type:            nonrobust                                                  \n",
              "==============================================================================\n",
              "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
              "------------------------------------------------------------------------------\n",
              "x1             0.2759      1.024      0.269      0.791      -1.885       2.436\n",
              "x2             0.1919      0.084      2.278      0.036       0.014       0.370\n",
              "x3            -0.0154      0.102     -0.151      0.882      -0.231       0.200\n",
              "==============================================================================\n",
              "Omnibus:                        0.745   Durbin-Watson:                   1.584\n",
              "Prob(Omnibus):                  0.689   Jarque-Bera (JB):                0.683\n",
              "Skew:                           0.110   Prob(JB):                        0.711\n",
              "Kurtosis:                       2.122   Cond. No.                         47.8\n",
              "==============================================================================\n",
              "\n",
              "Warnings:\n",
              "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
              "\"\"\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNn4l_BiS-iq"
      },
      "source": [
        "#Validation Metrics\n",
        "\n",
        "## F-Stat\n",
        "\n",
        "The F-test is a test where the null hypothesis that all the coefficients are 0 (eg. your model is no better than the mean). Of interest generally is the **p-value** on that test, which should always be be 0.00 -- otherwise your model has big problems.\n",
        "\n",
        "## Log likelihood\n",
        "\n",
        "The Log likelihood (LL) comes from the model's [likelihood function](https://en.wikipedia.org/wiki/Likelihood_function). Log-likelihood is all your dataset run through the pdf of the likelihood (normal distribution for OLS), and then they are summed together, and then taking the log of this sum.\n",
        "\n",
        "So it's measure of **model loss** in the sense that it's a difference between prediction likelihood and reality.\n",
        "\n",
        "The only real interpretation for log-likelihood is, higher is better.\n",
        "\n",
        "Log-likelihood values cannot be used alone as an index of model fit because they are a function of sample size but can be used to compare the fit of different coefficients.\n",
        "\n",
        "## AIC and BIC\n",
        "\n",
        "The [Akaike information criterion (AIC)](https://en.wikipedia.org/wiki/Akaike_information_criterion) is a **model selection criterion**. It's used to compare different models on the same dataset to compare them.\n",
        "\n",
        "The equation is $AIC = 2k - 2ln(\\hat{L})$ where $k$ is the number of coefficients and $L$ is the model likelihood stat. \n",
        "\n",
        "Generally you will compare models on the same dataset and pick the one with the smallest value, this is to penalize models which could be **overfitting** to the data.\n",
        "\n",
        "The formula for the Bayesian information criterion (BIC) is similar to the formula for AIC, but with a different penalty for the number of parameters. With AIC the penalty is $2k$, whereas with BIC the penalty is $ln(n)k$.\n",
        "\n",
        "## Durbin Watson\n",
        "\n",
        "The [Durbin-Watson](https://en.wikipedia.org/wiki/Durbin%E2%80%93Watson_statistic) test checks if the errors in your model have **autocorrelation** (which would imply heteroscedasticity). \n",
        "\n",
        "So it's a homoscedasticity test.\n",
        "\n",
        "##  Skew\n",
        "\n",
        "Skew is a statistic check for equality of dispersion in your model's error term. A distribution can have right or left-skew, but either way this disperses away from normality of errors, invalidating coefficients and standard errors.\n",
        "\n",
        "##  Omnibus\n",
        "\n",
        "A test of the skewness and kurtosis of the residual \n",
        "\n",
        "We hope to see a value close to zero which would indicate a normal distribution.\n",
        "\n",
        "The **Prob (Omnibus)** performs a statistical test indicating the probability that the residuals are normally distributed. We hope to see something close to 1 here. \n",
        "\n",
        "##  Kurtosis\n",
        "\n",
        "A measure of \"peakiness\", or curvature of the data. Higher peaks lead to greater Kurtosis. Greater Kurtosis can be interpreted as a tighter clustering of residuals around zero, implying a better model with few outliers.\n",
        "\n",
        "## Jarque-Bera (JB)\n",
        "\n",
        "Like the Omnibus test in that it tests both skew and kurtosis. We hope to see in this test a confirmation of the Omnibus test.\n",
        "\n",
        "## Condition Number\n",
        "\n",
        "This test measures the sensitivity of a function's output as compared to its input.\n",
        "\n",
        "When we have severe multicollinearity, we can expect much higher fluctuations to small changes in the data, hence, we hope to see a relatively small number, something below 30.\n",
        "\n",
        "When the $X^-2$ matrix blows up, the condition number will raise a warning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwBffK5wb0ao"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}